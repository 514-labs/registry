## Analytical Connectors Common Specification

Purpose-built integrations that extract data from source systems and load it into analytical stores (data warehouses or lakes) for reporting, modeling, and BI. They prioritize correctness, incremental delivery, and schema stability.

### Data Model
- Favor a normalized schema: entities (e.g., users, accounts), events (e.g., pageview, charge), and reference tables
- Required columns per table: primary key, `updated_at` (source clock), `ingested_at` (connector clock UTC)
- Prefer scalar columns; keep nested payloads in a `_raw` JSON column when needed

### Sync Semantics
- Support both initial full sync and ongoing incremental syncs
- Use a deterministic cursor (e.g., `updated_at`, `event_timestamp`, or CDC offset); CDC is preferred when available
- Perform idempotent loads via MERGE/UPSERT on primary key (and cursor where relevant)
- Chunk and paginate reads; stream writes to avoid unbounded memory

### Schema Evolution
- Backwards-compatible, additive by default; avoid breaking renames/drops
- Use stable, documented naming conventions (snake_case; UTC timestamps)
- Emit clear migration notes when columns are added or semantics change

### Data Quality and Deletes
- Deduplicate by primary key and the latest `updated_at`/version
- Represent soft deletes with `is_deleted` and `deleted_at`; propagate hard deletes when the source exposes them
- Validate basic types and required fields; route malformed rows to a dead-letter path/table

### Performance and Limits
- Respect source rate limits; use concurrency controls and adaptive backoff with jitter
- Use incremental checkpoints after each page/batch so jobs can resume safely
- Prefer server-side filtering and projection to minimize transfer size

### Observability
- Logs: structured, with job/run IDs and page/batch numbers; never log secrets
- Metrics: `rows_read`, `rows_written`, `lag_seconds`, `duplicate_rows`, `retries`, and `duration_seconds`
- Optional tracing spans around source fetch, transform, and load

### Security
- TLS by default; least-privilege access to sources and targets
- PII handling: configurable field redaction/masking; scrub sensitive data from logs and metrics

### Documentation
- List covered entities and their cursors, limitations/quotas, and expected sync cadences
- Provide example schemas, sample queries, and recovery steps for common failures